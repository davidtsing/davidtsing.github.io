<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="AlexNet," />










<meta name="description" content="2012-AlexNet主要特色包括：非线性拟合函数（ReLU）、防止过拟合（Dropout、数据增广）、大数据训练（百万级ImageNet图像数据）以及分Group实现双GPU并行、LRN归一化层。以下为AlexNet模型网络架构：">
<meta name="keywords" content="AlexNet">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习之TensorFlow（3）AlexNet模型">
<meta property="og:url" content="http://davidtsing.com/deep-learning-tensorflow-alexnet/index.html">
<meta property="og:site_name" content="大道三千">
<meta property="og:description" content="2012-AlexNet主要特色包括：非线性拟合函数（ReLU）、防止过拟合（Dropout、数据增广）、大数据训练（百万级ImageNet图像数据）以及分Group实现双GPU并行、LRN归一化层。以下为AlexNet模型网络架构：">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://davidtsing.com/deep-learning-tensorflow-alexnet/AlexNet.jpg">
<meta property="og:image" content="http://davidtsing.com/deep-learning-tensorflow-alexnet/Multiple-GPU.jpg">
<meta property="og:updated_time" content="2018-04-20T02:30:07.397Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习之TensorFlow（3）AlexNet模型">
<meta name="twitter:description" content="2012-AlexNet主要特色包括：非线性拟合函数（ReLU）、防止过拟合（Dropout、数据增广）、大数据训练（百万级ImageNet图像数据）以及分Group实现双GPU并行、LRN归一化层。以下为AlexNet模型网络架构：">
<meta name="twitter:image" content="http://davidtsing.com/deep-learning-tensorflow-alexnet/AlexNet.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://davidtsing.com/deep-learning-tensorflow-alexnet/"/>





  <title>深度学习之TensorFlow（3）AlexNet模型 | 大道三千</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">大道三千</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">人工智能（AI），机器学习，深度学习，面试算法，数学基础</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://davidtsing.com/deep-learning-tensorflow-alexnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="David Tsing">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大道三千">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习之TensorFlow（3）AlexNet模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-20T09:24:03+08:00">
                2018-04-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>2012-AlexNet主要特色包括：非线性拟合函数（ReLU）、防止过拟合（Dropout、数据增广）、大数据训练（百万级ImageNet图像数据）以及分Group实现双GPU并行、LRN归一化层。以下为AlexNet模型网络架构：<br> <img src="/deep-learning-tensorflow-alexnet/AlexNet.jpg" title="AlexNet.jpg"><br> <a id="more"></a></p>
<h1 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h1><p>AlexNet使用ReLU代替了传统的激活函数，而现在ReLU已经广泛地使用在了各种CNN结构中。关于ReLU这里已经没必要讲了，在2012年算是新颖的技术，到今天已经是遍地开花了。</p>
<h1 id="Multiple-GPU"><a href="#Multiple-GPU" class="headerlink" title="Multiple GPU"></a>Multiple GPU</h1><p>AlexNet使用两块GTX580显卡进行训练，两块显卡只需要在特定的层进行通信，当今的深度学习已经基本都是在显卡上完成的。两块GPU各训练网络的一部分，在第二个卷积层和全连接层才需要互相通信。<br>在卷积层之间还需要加上Local Response Normalization和Overlap Pooling。<br><img src="/deep-learning-tensorflow-alexnet/Multiple-GPU.jpg" title="Multiple-GPU.jpg"></p>
<h1 id="Local-Response-Normalization-LRN"><a href="#Local-Response-Normalization-LRN" class="headerlink" title="Local Response Normalization(LRN)"></a>Local Response Normalization(LRN)</h1><p>ReLU本来是不需要对输入进行标准化，但Alex发现进行局部标准化能提高性能。LRN处理类似于生物神经元的横向抑制机制，可以理解为将局部响应最大的再放大，并抑制其他响应较小的。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lrn</span><span class="params">(_x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.lrn(_x, depth_radius=<span class="number">4</span>, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="Overlap-Pooling"><a href="#Overlap-Pooling" class="headerlink" title="Overlap Pooling"></a>Overlap Pooling</h1><p>一般的Pooling是不重叠的，而AlexNet使用的Pooling是可重叠的，也就是说，在池化的时候，每次移动的步长小于池化的边长。AlexNet池化的大小为3 x 3的正方形，每次池化移动步长为2，这样就会出现重叠。</p>
<h1 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h1><p>AlexNet有6000万个参数，需要考虑过拟合的问题。</p>
<h2 id="数据增广"><a href="#数据增广" class="headerlink" title="数据增广"></a>数据增广</h2><p>对图片进行一些变换可以增大数据集。第一种方法是：原始图片大小为256 x 256，在图片上随机选取224 x 224的小块进行训练，还可以这些小块进行水平翻转进一步增加数据量。另一种方法是使用PCA改变训练图像RGB通道的像素值。</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>这个方法现在也是大量使用。Dropout将隐层输出以一定的概率置0，这个神经元就不参与前向和后向传播，就如同在网络中删除了一样。Dropout也可以看成是一种模型组合，每个样本都是不一样的网络结构。Dropout减少了神经元之间的共适应关系(co-adaptation)，一个神经元不再依赖另一个神经元，强迫网络学习到更加鲁棒的特征表示。</p>
<h1 id="AlexNet模型Tensorflow实现"><a href="#AlexNet模型Tensorflow实现" class="headerlink" title="AlexNet模型Tensorflow实现"></a>AlexNet模型Tensorflow实现</h1><p>TensorFlow官方的AlexNet实现，因为ImageNet数据集太大，所以官方是代码实现计算了前向和反向的训练时间，并没有真正的训练模型。下文在CIFAR10上测试了AlexNet的性能。CIFAR10数据集的图片展开为24 x 24 x 3，两个数据集上图片都较小，所有在卷积层和池化层的操作上都缩小了操作核的大小和步长，以保持图像的尺寸不会迅速崩塌。同样的以为CIFAR10的数据图像也不大，所以还是用简化版本的AlexNet。</p>
<h2 id="第一层：卷积层（输入–-gt-卷积–-gt-ReLUs–-gt-LRN–-gt-max-pooling）"><a href="#第一层：卷积层（输入–-gt-卷积–-gt-ReLUs–-gt-LRN–-gt-max-pooling）" class="headerlink" title="第一层：卷积层（输入–&gt;卷积–&gt;ReLUs–&gt;LRN–&gt;max-pooling）"></a>第一层：卷积层（输入–&gt;卷积–&gt;ReLUs–&gt;LRN–&gt;max-pooling）</h2><p>输入图片的尺寸为[227x227x3]，经过第一卷积层之后96 11x11 filters at stride 4, pad 0 $\frac{227-11}{4} + 1 = 55$得到尺寸[55x55x96]，继续导入Max pooling1 3x3 filters at stride 2 $\frac{55-3}{2} + 1 = 27$得到[27x27x96]尺寸。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Layer1 - Convolution (Input–&gt;Conv–&gt;ReLUs–&gt;LRN–&gt;max-pooling)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1-conv1'</span>):</span><br><span class="line">	conv1_weights = tf.get_variable(<span class="string">'weight'</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>],initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">	conv1_biases = tf.get_variable(<span class="string">'bias'</span>,[<span class="number">32</span>],initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">	conv1 = conv2d(image_holder, conv1_weights, conv1_biases)</span><br><span class="line">	lrn1 = lrn(conv1)</span><br><span class="line">	pool1 = max_pool(lrn1, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="第二层：卷积层（卷积–-gt-ReLUs–-gt-LRN–-gt-max-pooling）"><a href="#第二层：卷积层（卷积–-gt-ReLUs–-gt-LRN–-gt-max-pooling）" class="headerlink" title="第二层：卷积层（卷积–&gt;ReLUs–&gt;LRN–&gt;max-pooling）"></a>第二层：卷积层（卷积–&gt;ReLUs–&gt;LRN–&gt;max-pooling）</h2><p>Max pooling1作为输入，经过第二卷积层256 5x5 filters at stride 1, pad 2 $\frac{27-5 + 2*2}{1} + 1 = 27$输出为[27x27x256]。将其导入Max pooling2中，3x3 filters at stride 2 $\frac{27 -3}{2} + 1 = 13$输出尺寸[13x13x256]。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Layer2 - Convolution(Conv–&gt;ReLUs–&gt;LRN–&gt;max-pooling)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer2-conv2'</span>):</span><br><span class="line">	conv2_weights = tf.get_variable(<span class="string">'weight'</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">64</span>],initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">	conv2_biases = tf.get_variable(<span class="string">'bias'</span>,[<span class="number">64</span>],initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">	conv2 = conv2d(pool1, conv2_weights, conv2_biases)</span><br><span class="line">	lrn2 = lrn(conv2)</span><br><span class="line">	pool2 = max_pool(lrn2, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="第三层：卷积层（卷积–-gt-ReLUs）"><a href="#第三层：卷积层（卷积–-gt-ReLUs）" class="headerlink" title="第三层：卷积层（卷积–&gt;ReLUs）"></a>第三层：卷积层（卷积–&gt;ReLUs）</h2><p>Max pooling2作为输入，流入第三卷积层384 3x3 filters at stride 1, pad 1 $\frac{13-3 + 2*1}{1} + 1 = 13$输出尺寸[13x13x384]。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Layer3 - Convolution(Conv–&gt;ReLUs)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer3-conv3'</span>):</span><br><span class="line">	conv3_weights = tf.get_variable(<span class="string">'weight'</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>],initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">	conv3_biases = tf.get_variable(<span class="string">'bias'</span>,[<span class="number">64</span>],initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">	conv3 = conv2d(pool2, conv3_weights, conv3_biases)</span><br></pre></td></tr></table></figure></p>
<h2 id="第四层：卷积层（卷积–-gt-ReLUs）"><a href="#第四层：卷积层（卷积–-gt-ReLUs）" class="headerlink" title="第四层：卷积层（卷积–&gt;ReLUs）"></a>第四层：卷积层（卷积–&gt;ReLUs）</h2><p>Conv3作为输入，流入第四卷积层384 3x3 filters at stride 1, pad 1 $\frac{13-3 + 2*1}{1} + 1 = 13$输出尺寸[13x13x384]。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Layer4 - Convolution(Conv–&gt;ReLUs)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer4-conv4'</span>):</span><br><span class="line">	conv4_weights = tf.get_variable(<span class="string">'weight'</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">256</span>],initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">	conv4_biases = tf.get_variable(<span class="string">'bias'</span>,[<span class="number">256</span>],initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">	conv4 = conv2d(conv3, conv4_weights, conv4_biases)</span><br></pre></td></tr></table></figure></p>
<h2 id="第五层：卷积层（卷积–-gt-ReLUs）"><a href="#第五层：卷积层（卷积–-gt-ReLUs）" class="headerlink" title="第五层：卷积层（卷积–&gt;ReLUs）"></a>第五层：卷积层（卷积–&gt;ReLUs）</h2><p>Conv4作为输入，流入第四卷积层256 3x3 filters at stride 1, pad 1 $\frac{13-3 + 2*1}{1} + 1 = 13$获得图像[13x13x256]，再经过Max pooling池化3x3 filters at stride 2 $\frac{13-3}{2} + 1 = 6$输出尺寸[6x6x256]。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Layer5 - Convolution(Conv–&gt;ReLUs-&gt;pooling)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer5-conv5'</span>):</span><br><span class="line">	conv5_weights = tf.get_variable(<span class="string">'weight'</span>,[<span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">128</span>],initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">	conv5_biases = tf.get_variable(<span class="string">'bias'</span>,[<span class="number">128</span>],initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">	conv5 = conv2d(conv4, conv5_weights, conv5_biases)</span><br><span class="line">	pool5 = max_pool(conv5, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="第六层：全连接层（矩阵乘法–-gt-ReLUs–-gt-dropout）"><a href="#第六层：全连接层（矩阵乘法–-gt-ReLUs–-gt-dropout）" class="headerlink" title="第六层：全连接层（矩阵乘法–&gt;ReLUs–&gt;dropout）"></a>第六层：全连接层（矩阵乘法–&gt;ReLUs–&gt;dropout）</h2><p>原文这里叫做池化层，但是由于输入输出一样，这里直接就称为全连接层了。Conv5作为输入，拉伸为向量，进行全连接，非线性映射，再丢掉一部分数据，最后输出[4096]个神经元。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Layer6 - Full Connection(pooling-&gt;vector-&gt;matmul–&gt;ReLUs–&gt;dropout)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer6-fc1'</span>):</span><br><span class="line">	fc1_weights = tf.get_variable(<span class="string">'weight'</span>,[<span class="number">128</span>*<span class="number">24</span>*<span class="number">24</span>, <span class="number">1024</span>],initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">	fc1_biases = tf.get_variable(<span class="string">'bias'</span>,[<span class="number">1024</span>],initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">	shape = pool5.get_shape()</span><br><span class="line">	reshape = tf.reshape(pool5, [<span class="number">-1</span>, shape[<span class="number">1</span>].value*shape[<span class="number">2</span>].value*shape[<span class="number">3</span>].value])</span><br><span class="line">	fc1 = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)</span><br><span class="line">	fc1_drop = tf.nn.dropout(fc1, keep_prob=dropout)</span><br></pre></td></tr></table></figure></p>
<h2 id="第七层：全连接层（矩阵乘法–-gt-ReLUs–-gt-dropout）"><a href="#第七层：全连接层（矩阵乘法–-gt-ReLUs–-gt-dropout）" class="headerlink" title="第七层：全连接层（矩阵乘法–&gt;ReLUs–&gt;dropout）"></a>第七层：全连接层（矩阵乘法–&gt;ReLUs–&gt;dropout）</h2><p>使用fc1_drop作为输入，进行全连接，非线性映射，再丢掉一部分数据，最后得到[4096]个神经元的输出。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Layer7 - Full Connection((matmul–&gt;ReLUs–&gt;dropout)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer7-fc2'</span>):</span><br><span class="line">	fc2_weights = tf.get_variable(<span class="string">'weight'</span>,[<span class="number">1024</span>, <span class="number">1024</span>],initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">	fc2_biases = tf.get_variable(<span class="string">'bias'</span>,[<span class="number">1024</span>],initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">	fc2 = tf.nn.relu(tf.matmul(fc1_drop, fc2_weights) + fc2_biases)</span><br><span class="line">	fc2_drop = tf.nn.dropout(fc2, keep_prob=dropout)</span><br></pre></td></tr></table></figure></p>
<h2 id="第八层：全连接层（矩阵乘法–-gt-ReLUs–-gt-softmax）"><a href="#第八层：全连接层（矩阵乘法–-gt-ReLUs–-gt-softmax）" class="headerlink" title="第八层：全连接层（矩阵乘法–&gt;ReLUs–&gt;softmax）"></a>第八层：全连接层（矩阵乘法–&gt;ReLUs–&gt;softmax）</h2><p>使用fc2_drop作为输入，进行全连接，非线性映射，再经过softmax分类，最终得到[1000]个类别。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Layer8 - Full Connection((matmul–&gt;ReLUs–&gt;softmax)</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'layer8-fc3'</span>):</span><br><span class="line">	fc3_weights = tf.get_variable(<span class="string">'weight'</span>,[<span class="number">1024</span>, <span class="number">10</span>],initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">	fc3_biases = tf.get_variable(<span class="string">'bias'</span>,[<span class="number">10</span>],initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">	logits = tf.add(tf.matmul(fc2_drop, fc3_weights) , fc3_biases)</span><br></pre></td></tr></table></figure></p>
<hr>
<p>References:<br>[1] <a href="https://blog.csdn.net/tinyzhao/article/details/53035944" target="_blank" rel="noopener">神经网络：AlexNet</a><br>[2] <a href="https://blog.csdn.net/u011974639/article/details/76146822" target="_blank" rel="noopener">TensorFlow实战：Chapter-4（CNN-2-经典卷积神经网络（AlexNet、VGGNet））</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/AlexNet/" rel="tag"># AlexNet</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/deep-learning-tensorflow-lenet5/" rel="next" title="深度学习之TensorFlow（2）LeNet模型">
                <i class="fa fa-chevron-left"></i> 深度学习之TensorFlow（2）LeNet模型
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">David Tsing</p>
              <p class="site-description motion-element" itemprop="description">主要研究人工智能，机器学习，深度学习，面试算法，数学基础...涉猎一些分布式与大数据，区块链内容。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/davidtsing" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:davidtsing@aliyun.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ReLU"><span class="nav-number">1.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multiple-GPU"><span class="nav-number">2.</span> <span class="nav-text">Multiple GPU</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Local-Response-Normalization-LRN"><span class="nav-number">3.</span> <span class="nav-text">Local Response Normalization(LRN)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Overlap-Pooling"><span class="nav-number">4.</span> <span class="nav-text">Overlap Pooling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#防止过拟合"><span class="nav-number">5.</span> <span class="nav-text">防止过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据增广"><span class="nav-number">5.1.</span> <span class="nav-text">数据增广</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout"><span class="nav-number">5.2.</span> <span class="nav-text">Dropout</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AlexNet模型Tensorflow实现"><span class="nav-number">6.</span> <span class="nav-text">AlexNet模型Tensorflow实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#第一层：卷积层（输入–-gt-卷积–-gt-ReLUs–-gt-LRN–-gt-max-pooling）"><span class="nav-number">6.1.</span> <span class="nav-text">第一层：卷积层（输入–&gt;卷积–&gt;ReLUs–&gt;LRN–&gt;max-pooling）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第二层：卷积层（卷积–-gt-ReLUs–-gt-LRN–-gt-max-pooling）"><span class="nav-number">6.2.</span> <span class="nav-text">第二层：卷积层（卷积–&gt;ReLUs–&gt;LRN–&gt;max-pooling）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第三层：卷积层（卷积–-gt-ReLUs）"><span class="nav-number">6.3.</span> <span class="nav-text">第三层：卷积层（卷积–&gt;ReLUs）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第四层：卷积层（卷积–-gt-ReLUs）"><span class="nav-number">6.4.</span> <span class="nav-text">第四层：卷积层（卷积–&gt;ReLUs）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第五层：卷积层（卷积–-gt-ReLUs）"><span class="nav-number">6.5.</span> <span class="nav-text">第五层：卷积层（卷积–&gt;ReLUs）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第六层：全连接层（矩阵乘法–-gt-ReLUs–-gt-dropout）"><span class="nav-number">6.6.</span> <span class="nav-text">第六层：全连接层（矩阵乘法–&gt;ReLUs–&gt;dropout）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第七层：全连接层（矩阵乘法–-gt-ReLUs–-gt-dropout）"><span class="nav-number">6.7.</span> <span class="nav-text">第七层：全连接层（矩阵乘法–&gt;ReLUs–&gt;dropout）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第八层：全连接层（矩阵乘法–-gt-ReLUs–-gt-softmax）"><span class="nav-number">6.8.</span> <span class="nav-text">第八层：全连接层（矩阵乘法–&gt;ReLUs–&gt;softmax）</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">David Tsing</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
